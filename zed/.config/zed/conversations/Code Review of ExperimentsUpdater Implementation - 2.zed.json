{"id":"ffae3c23-52f5-4618-a151-b774dd692918","zed":"context","version":"0.4.0","text":"please code review \n\ntype ExperimentsUpdater struct {\n\tusersProvider     *PhantomEnabledUserIDs\n\tupdate            []string\n\tconfigAppEndpoint string\n\tc                 *http.Client\n}\n\nfunc NewGroupUpdater(client *http.Client, up *PhantomEnabledUserIDs, url string) *ExperimentsUpdater {\n\treturn &ExperimentsUpdater{\n\t\tusersProvider:     up,\n\t\tc:                 client,\n\t\tconfigAppEndpoint: url,\n\t}\n}\n\nfunc (e *ExperimentsUpdater) Run(ctx context.Context) {\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase <-time.After(300 * time.Second):\n\t\t\terr := e.updateExperiments(ctx)\n\t\t\tif err != nil {\n\t\t\t\tlog.Error(\"failed to update experiments\", log.Err(err))\n\t\t\t}\n\t\t\te.update = nil\n\t\t}\n\n\t}\n}\n\nfunc (e *ExperimentsUpdater) AddUser(id string) {\n\te.update = append(e.update, id)\n}\n\nfunc (e *ExperimentsUpdater) updateExperiments(ctx context.Context) error {\n\tupdate := append(e.usersProvider.GetIDs(), e.update...)\n\tdata, err := json.MarshalIndent(update, \"\", \"  \")\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treq, err := http.NewRequestWithContext(ctx, http.MethodPost, e.configAppEndpoint, bytes.NewBuffer(data))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treq.Header.Set(\"X-Email\", \"usercom.react@aviasales.ru\")\n\treq.Header.Set(\"Content-Type\", \"application/json\")\n\n\tresp, err := e.c.Do(req)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer resp.Body.Close()\n\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn fmt.Errorf(\"unexpected status code: %d\", resp.StatusCode)\n\t}\n\n\treturn nil\n}\n\n// somewhere in main()\n\n\tgroupUpdater := NewGroupUpdater(&http.Client{Transport: transport.Default()},\n\t\tphantomUsers,\n\t\tcfg.RemoteConfigs.URL,\n\t)\n\tgo func() {\n\t\tgroupUpdater.Run(context.Background())\n\t}()\n// somewhere in the application logic\nif !phantomUsers.Contains(userID) {\n\t\t\tlogger.Debug(\"user_id does not belong to an experiment group\")\n\t\t\tchance := rand.New(rand.NewSource(time.Now().UnixNano())).Float64() * 100\n\t\t\tif chance < cfg.PhantomEnablePercentage.Percentage {\n\t\t\t\tlogger.Info(\"adding user to experiment group\")\n\t\t\t\tgroupUpdater.AddUser(userID)\n\t\t\t}\n\n\t\t\treturn nil\n\t\t}\n```go delta/price-alerts/cmd/phantom-subscriptions-creator/main.go\npackage main\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"math/rand\"\n\t\"net/http\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/KosyanMedia/delta/pkg/datetime\"\n\t\"github.com/KosyanMedia/delta/pkg/format/message/datacollection/aggregates\"\n\t\"github.com/KosyanMedia/delta/pkg/iata\"\n\t\"github.com/KosyanMedia/delta/pkg/usercom/app_service\"\n\t\"github.com/KosyanMedia/delta/pkg/usercom/cmd\"\n\t\"github.com/KosyanMedia/delta/pkg/usercom/configs\"\n\t\"github.com/KosyanMedia/delta/pkg/usercom/remote_configs\"\n\t\"github.com/KosyanMedia/delta/pkg/usercom/subscriptions\"\n\t\"github.com/KosyanMedia/delta/pkg/usercom/types\"\n\tsubscriptions_types \"github.com/KosyanMedia/delta/pkg/usercom/types/subscriptions\"\n\tpakafka \"github.com/KosyanMedia/delta/price-alerts/internal/kafka\"\n\t\"github.com/KosyanMedia/gopkg/encoding/json\"\n\t\"github.com/KosyanMedia/gopkg/http/client/v2/transport\"\n\t\"github.com/KosyanMedia/gopkg/kafka\"\n\t\"github.com/KosyanMedia/gopkg/log\"\n)\n\nconst twoWeeks = 24 * time.Hour * 7 * 2\n\ntype Config struct {\n\tCommon        configs.CommonConfig\n\tRemoteConfigs configs.RemoteConfigsConfig\n\tKafkaAuth     configs.KafkaAuthConfig\n\tConsumer      struct {\n\t\tBrokers string `env:\"CONSUMER_KAFKA_BROKERS,required\"`\n\t\tTopic   string `env:\"CONSUMER_KAFKA_TOPIC,required\"`\n\t\tGroupID string `env:\"CONSUMER_KAFKA_GROUP_ID,required\"`\n\t}\n\tSubscriptionsAPI struct {\n\t\tEndpoint string `env:\"SUBSCRIPTIONS_API_ENDPOINT,required\"`\n\t}\n\tPhantomEnablePercentage struct {\n\t\tPercentage float64 `env:\"PHANTOM_ENABLE_PERCENTAGE,required\" envdefault:\"50.0\"`\n\t}\n}\n\ntype PhantomEnabledUserIDs struct {\n\tcfg atomic.Pointer[UserIDs]\n}\n\ntype UserIDs struct {\n\tIDs map[string]struct{}\n}\n\nfunc NewPhantomEnabledUserIDs() *PhantomEnabledUserIDs {\n\tp := &PhantomEnabledUserIDs{}\n\tp.cfg.Store(&UserIDs{IDs: make(map[string]struct{})})\n\treturn p\n}\n\nfunc (c *PhantomEnabledUserIDs) SetConfig(b []byte) error {\n\tvar ids []string\n\tif err := json.Unmarshal(b, &ids); err != nil {\n\t\treturn fmt.Errorf(\"parse user_ids config unmarshal json error: %w\", err)\n\t}\n\n\tconfig := &UserIDs{IDs: make(map[string]struct{}, len(ids))}\n\tfor _, id := range ids {\n\t\tconfig.IDs[id] = struct{}{}\n\t}\n\tc.cfg.Store(config)\n\n\treturn nil\n}\n\nfunc (c *PhantomEnabledUserIDs) Contains(userID string) bool {\n\tuserIDs := c.cfg.Load()\n\t_, ok := userIDs.IDs[userID]\n\treturn ok\n}\n\nfunc (c *PhantomEnabledUserIDs) GetIDs() []string {\n\tuserIDs := c.cfg.Load()\n\tvar ids []string\n\tfor id := range userIDs.IDs {\n\t\tids = append(ids, id)\n\t}\n\treturn ids\n}\n\ntype ExperimentsUpdater struct {\n\tusersProvider     *PhantomEnabledUserIDs\n\tupdate            []string\n\tconfigAppEndpoint string\n\tc                 *http.Client\n}\n\nfunc NewGroupUpdater(client *http.Client, up *PhantomEnabledUserIDs, url string) *ExperimentsUpdater {\n\treturn &ExperimentsUpdater{\n\t\tusersProvider:     up,\n\t\tc:                 client,\n\t\tconfigAppEndpoint: url,\n\t}\n}\n\nfunc (e *ExperimentsUpdater) Run(ctx context.Context) {\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase <-time.After(300 * time.Second):\n\t\t\terr := e.updateExperiments(ctx)\n\t\t\tif err != nil {\n\t\t\t\tlog.Error(\"failed to update experiments\", log.Err(err))\n\t\t\t}\n\t\t\te.update = nil\n\t\t}\n\n\t}\n}\n\nfunc (e *ExperimentsUpdater) AddUser(id string) {\n\te.update = append(e.update, id)\n}\n\nfunc (e *ExperimentsUpdater) updateExperiments(ctx context.Context) error {\n\tupdate := append(e.usersProvider.GetIDs(), e.update...)\n\tdata, err := json.MarshalIndent(update, \"\", \"  \")\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treq, err := http.NewRequestWithContext(ctx, http.MethodPost, e.configAppEndpoint, bytes.NewBuffer(data))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treq.Header.Set(\"X-Email\", \"usercom.react@aviasales.ru\")\n\treq.Header.Set(\"Content-Type\", \"application/json\")\n\n\tresp, err := e.c.Do(req)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer resp.Body.Close()\n\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn fmt.Errorf(\"unexpected status code: %d\", resp.StatusCode)\n\t}\n\n\treturn nil\n}\n\nfunc main() {\n\tvar cfg Config\n\tconfigs.MustParseContainer(&cfg)\n\n\tcmd.Initialize(cfg.Common)\n\n\tremoteConfigs := remote_configs.New(cfg.Common.App, cfg.RemoteConfigs)\n\tphantomUsers := NewPhantomEnabledUserIDs()\n\tremoteConfigs.On(\"usercom/phantom_subscriptions_user_ids\", phantomUsers.SetConfig)\n\n\tgroupUpdater := NewGroupUpdater(&http.Client{Transport: transport.Default()},\n\t\tphantomUsers,\n\t\tcfg.RemoteConfigs.URL,\n\t)\n\tgo func() {\n\t\tgroupUpdater.Run(context.Background())\n\t}()\n\n\tsubscriptionsAPI := subscriptions.NewClient(\n\t\t&http.Client{Transport: transport.Default()},\n\t\tcfg.SubscriptionsAPI.Endpoint,\n\t\tfalse,\n\t)\n\n\tconsumercfg := kafka.NewConsumerConfig(\"usercom.phantom-subscriptions-creator.aggregates-consumer\")\n\tif cfg.KafkaAuth.IsEnabled {\n\t\tconsumercfg.SASL(cfg.KafkaAuth.Username, cfg.KafkaAuth.Password)\n\t\tconsumercfg.TLSCert([]byte(cfg.KafkaAuth.PemCert))\n\t}\n\n\tconsumer := pakafka.NewConsumer(cfg.Consumer.Brokers,\n\t\tcfg.Consumer.GroupID,\n\t\tcfg.Consumer.Topic,\n\t\tconsumercfg,\n\t)\n\n\tconsumer.C.HandlePartitionMessage(func(message *kafka.ConsumerMessage, session kafka.ConsumerGroupSession) error {\n\t\tdefer session.MarkMessage(message)\n\n\t\tvar sr aggregates.SearchChainPayload\n\t\tif err := json.Unmarshal(message.Value, &sr); err != nil {\n\t\t\tlog.Error(\"could not unmarshal search result\", log.Err(err))\n\t\t\treturn err\n\t\t}\n\n\t\tif sr.SearchParams == nil || sr.SearchParams.UserData == nil || sr.SearchParams.UserData.Id == \"\" {\n\t\t\treturn nil\n\t\t}\n\n\t\tuserID := sr.SearchParams.UserData.Id\n\t\tlogger := log.With(log.String(\"user_id\", userID))\n\n\t\tif !phantomUsers.Contains(userID) {\n\t\t\tlogger.Debug(\"user_id does not belong to an experiment group\")\n\t\t\tchance := rand.New(rand.NewSource(time.Now().UnixNano())).Float64() * 100\n\t\t\tif chance < cfg.PhantomEnablePercentage.Percentage {\n\t\t\t\tlogger.Info(\"adding user to experiment group\")\n\t\t\t\tgroupUpdater.AddUser(userID)\n\t\t\t}\n\n\t\t\treturn nil\n\t\t}\n\n\t\tsubs, err := subscriptionsAPI.FetchUserSubscriptions(context.Background(), userID, types.BrandAviasales)\n\t\tif err != nil {\n\t\t\tlogger.Error(\"could not fetch user subscriptions\", log.Err(err))\n\t\t\treturn err\n\t\t}\n\t\tlogger.Info(\"received user subscriptions\", log.Int(\"subscriptions_count\", len(subs)))\n\n\t\tduplicatedID, err := checkDuplicates(subs, sr)\n\t\tif err != nil {\n\t\t\tlogger.Error(\"could not check duplicates\", log.Err(err))\n\t\t\treturn err\n\t\t}\n\t\tif duplicatedID != \"\" {\n\t\t\tlogger.Info(\"skipping search: subscription with such segments already exists\",\n\t\t\t\tlog.String(\"subscription_id\", duplicatedID),\n\t\t\t)\n\t\t\treturn nil\n\t\t}\n\n\t\tok, err := lastNotificationThresholdPassed(subscriptionsAPI, userID)\n\t\tif err != nil {\n\t\t\tlogger.Error(\"could not check last notification threshold\", log.Err(err))\n\t\t\treturn err\n\t\t}\n\t\tif !ok {\n\t\t\tlogger.Info(\"skipping search: less than 2 weeks passed since the last notification\")\n\t\t\treturn nil\n\t\t}\n\n\t\tcheck, err := maybeInvalidatePhantomSub(userID, subs, subscriptionsAPI)\n\t\tif err != nil {\n\t\t\tlogger.Error(\"could not invalidate phantom subscription\", log.Err(err))\n\t\t\treturn err\n\t\t}\n\t\tif check != nil {\n\t\t\tif !check.Invalidated {\n\t\t\t\tlogger.Info(\"skipping search: active phantom subscription already exists\",\n\t\t\t\t\tlog.String(\"active_phantom_subscription_id\", check.ActivePhantomID),\n\t\t\t\t)\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\tlogger.Info(\"invalidated phantom subscription\",\n\t\t\t\tlog.String(\"deleted_phantom_subscription_id\", check.DeletedPhantomID),\n\t\t\t)\n\t\t}\n\n\t\tif cerr := createPhantomSub(subscriptionsAPI, sr); cerr != nil {\n\t\t\tlogger.Error(\"could not create phantom subscription\", log.Err(cerr))\n\t\t\treturn cerr\n\t\t}\n\n\t\treturn nil\n\t})\n\n\tlauncher := app_service.NewLauncher(consumer, remoteConfigs)\n\tif err := launcher.Run(); err != nil {\n\t\tlog.Fatal(\"app start error\", log.Err(err))\n\t}\n}\n\nfunc createPhantomSub(api *subscriptions.Client, sr aggregates.SearchChainPayload) error {\n\treq := subscriptions_types.PostPhantomSubscriptionRequest{\n\t\tUserID:    sr.SearchParams.UserData.Id,\n\t\tTripClass: iata.NewTripClass(sr.SearchParams.TripClass.String()),\n\t}\n\n\tif p := sr.SearchParams.Passengers; p != nil {\n\t\treq.Adults = uint8(p.Adults)\n\t}\n\n\tif ua := sr.SearchParams.UserData.UserAgent; ua != nil {\n\t\treq.Marker = ua.Marker\n\t}\n\tsegments, err := getSegments(sr)\n\tif err != nil {\n\t\treturn err\n\t}\n\treq.Segments = segments\n\n\treturn api.PostPhantomSubscription(context.Background(), req)\n}\n\ntype InvalidationCheck struct {\n\tInvalidated      bool\n\tDeletedPhantomID string\n\tActivePhantomID  string\n}\n\nfunc maybeInvalidatePhantomSub(userID string, subs []*subscriptions_types.Subscription, api *subscriptions.Client) (*InvalidationCheck, error) {\n\tnow := time.Now()\n\tfor _, sub := range subs {\n\t\tif sub.Type != subscriptions_types.SubscriptionTypePhantomDirect {\n\t\t\tcontinue\n\t\t}\n\n\t\tif now.Sub(sub.CreatedAt) <= 3*time.Hour {\n\t\t\tif err := api.DeletePhantomSubscriptions(context.Background(), []string{userID}); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\treturn &InvalidationCheck{\n\t\t\t\tInvalidated:      true,\n\t\t\t\tDeletedPhantomID: sub.ID,\n\t\t\t}, nil\n\t\t}\n\n\t\treturn &InvalidationCheck{\n\t\t\tInvalidated:     false,\n\t\t\tActivePhantomID: sub.ID,\n\t\t}, nil\n\t}\n\n\treturn nil, nil\n}\n\nfunc lastNotificationThresholdPassed(api *subscriptions.Client, userID string) (bool, error) {\n\tnow := time.Now()\n\tuserData, err := api.GetUserData(context.Background(), userID)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\n\tif userData == nil {\n\t\treturn false, fmt.Errorf(\"no user data found: %s\", userID)\n\t}\n\n\tif userData.LastPhantomNotifiedAt.IsZero() {\n\t\treturn true, nil\n\t}\n\n\tif now.Sub(userData.LastPhantomNotifiedAt) <= twoWeeks {\n\t\treturn false, nil\n\t}\n\n\treturn true, nil\n}\n\nfunc checkDuplicates(subs []*subscriptions_types.Subscription, sr aggregates.SearchChainPayload) (string, error) {\n\tsegments, err := getSegments(sr)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tfor _, sub := range subs {\n\t\tif equalSegments(sub.Segments, segments) {\n\t\t\treturn sub.ID, nil\n\t\t}\n\t}\n\n\treturn \"\", nil\n}\n\nfunc getSegments(sr aggregates.SearchChainPayload) ([]subscriptions_types.RouteSegment, error) {\n\tsegments := make([]subscriptions_types.RouteSegment, 0, len(sr.SearchParams.Segments))\n\tfor _, segment := range sr.SearchParams.Segments {\n\t\tdate, err := datetime.ParseDate(segment.Date)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tsegments = append(segments, subscriptions_types.RouteSegment{\n\t\t\tOriginCity:      segment.Origin,\n\t\t\tDestinationCity: segment.Destination,\n\t\t\tDepartureDate:   date,\n\t\t})\n\t}\n\treturn segments, nil\n}\n\nfunc equalSegments(s1, s2 []subscriptions_types.RouteSegment) bool {\n\tif len(s1) != len(s2) {\n\t\treturn false\n\t}\n\tn := len(s1)\n\tfor i := 0; i < n; i++ {\n\t\ta := s1[i]\n\t\tb := s2[i]\n\t\tif a.OriginCity != b.OriginCity {\n\t\t\treturn false\n\t\t}\n\t\tif a.DestinationCity != b.DestinationCity {\n\t\t\treturn false\n\t\t}\n\t\tif a.DepartureDate != b.DepartureDate {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n```\n\n\n# Code Review: ExperimentsUpdater\n\n## Overview\nThe code manages a system for tracking users in experimental groups and updating this information remotely.\n\n## Strengths\n- Separation of concerns between user tracking and updating functionality\n- Proper use of contexts for cancellation\n- Good error handling in the HTTP request flow\n- Thread-safety with atomic pointer for user IDs\n\n## Issues and Recommendations\n\n### 1. Race Condition in `AddUser`\n```go\nfunc (e *ExperimentsUpdater) AddUser(id string) {\n    e.update = append(e.update, id)\n}\n```\n**Issue**: This method is not thread-safe. If multiple goroutines call `AddUser` concurrently, it could cause race conditions.\n\n**Recommendation**: Add a mutex to protect access to the `update` slice:\n```go\ntype ExperimentsUpdater struct {\n    usersProvider     *PhantomEnabledUserIDs\n    update            []string\n    configAppEndpoint string\n    c                 *http.Client\n    mu                sync.Mutex // Add mutex\n}\n\nfunc (e *ExperimentsUpdater) AddUser(id string) {\n    e.mu.Lock()\n    defer e.mu.Unlock()\n    e.update = append(e.update, id)\n}\n```\n\n### 2. Clearing `update` Slice \n```go\nfunc (e *ExperimentsUpdater) Run(ctx context.Context) {\n    // ...\n    e.update = nil // after updating experiments\n    // ...\n}\n```\n**Issue**: This assignment is not thread-safe and should be protected by the same mutex.\n\n**Recommendation**: \n```go\ne.mu.Lock()\ne.update = nil\ne.mu.Unlock()\n```\n\n### 3. Accessing `update` in `updateExperiments`\n```go\nfunc (e *ExperimentsUpdater) updateExperiments(ctx context.Context) error {\n    update := append(e.usersProvider.GetIDs(), e.update...)\n    // ...\n}\n```\n\n**Issue**: The `e.update` slice is accessed without protection.\n\n**Recommendation**: \n```go\nfunc (e *ExperimentsUpdater) updateExperiments(ctx context.Context) error {\n    e.mu.Lock()\n    update := append(e.usersProvider.GetIDs(), e.update...)\n    e.mu.Unlock()\n    // ...\n}\n```\n\n### 4. Potential Duplicate User IDs\n**Issue**: There's no mechanism to prevent duplicate IDs in the `update` slice.\n\n**Recommendation**: Consider using a map to deduplicate before updating:\n```go\nfunc (e *ExperimentsUpdater) updateExperiments(ctx context.Context) error {\n    e.mu.Lock()\n    // Use a map to deduplicate\n    uniqueIDs := make(map[string]struct{})\n    for _, id := range e.usersProvider.GetIDs() {\n        uniqueIDs[id] = struct{}{}\n    }\n    for _, id := range e.update {\n        uniqueIDs[id] = struct{}{}\n    }\n    \n    // Convert back to slice\n    var update []string\n    for id := range uniqueIDs {\n        update = append(update, id)\n    }\n    e.mu.Unlock()\n    \n    data, err := json.MarshalIndent(update, \"\", \"  \")\n    // ...\n}\n```\n\n### 5. Magic Constants\n**Issue**: The code uses a hard-coded 300-second (5-minute) interval.\n\n**Recommendation**: Make this configurable:\n```go\ntype ExperimentsUpdater struct {\n    // ...\n    updateInterval time.Duration\n}\n\nfunc NewGroupUpdater(client *http.Client, up *PhantomEnabledUserIDs, url string, interval time.Duration) *ExperimentsUpdater {\n    return &ExperimentsUpdater{\n        usersProvider:     up,\n        c:                 client,\n        configAppEndpoint: url,\n        updateInterval:    interval,\n    }\n}\n\nfunc (e *ExperimentsUpdater) Run(ctx context.Context) {\n    for {\n        select {\n        case <-ctx.Done():\n            return\n        case <-time.After(e.updateInterval):\n            // ...\n        }\n    }\n}\n```\n\n### 6. Running the Updater\n```go\ngo func() {\n    groupUpdater.Run(context.Background())\n}()\n```\n\n**Issue**: Using a separate `context.Background()` means this goroutine won't terminate properly if the main process is shutting down.\n\n**Recommendation**: Use a shared context from the parent:\n```go\nctx, cancel := context.WithCancel(context.Background())\ndefer cancel()\n\ngo func() {\n    groupUpdater.Run(ctx)\n}()\n```\n\n### 7. Random Number Generation\n```go\nchance := rand.New(rand.NewSource(time.Now().UnixNano())).Float64() * 100\n```\n\n**Issue**: Creating a new random source for each check is inefficient.\n\n**Recommendation**: Create a single global random source using `rand.NewSource()` during initialization and reuse it, or use the newer more secure `rand/v2` package.\n\n## Summary\nThe core logic is sound, but there are concurrency issues that need addressing. Adding proper mutex protection and considering the other recommendations would make this code more robust for production use.\n```go delta/price-alerts/cmd/phantom-subscriptions-creator/main.go\npackage main\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"math/rand/v2\"\n\t\"net/http\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/KosyanMedia/delta/pkg/datetime\"\n\t\"github.com/KosyanMedia/delta/pkg/format/message/datacollection/aggregates\"\n\t\"github.com/KosyanMedia/delta/pkg/iata\"\n\t\"github.com/KosyanMedia/delta/pkg/usercom/app_service\"\n\t\"github.com/KosyanMedia/delta/pkg/usercom/cmd\"\n\t\"github.com/KosyanMedia/delta/pkg/usercom/configs\"\n\t\"github.com/KosyanMedia/delta/pkg/usercom/remote_configs\"\n\t\"github.com/KosyanMedia/delta/pkg/usercom/subscriptions\"\n\t\"github.com/KosyanMedia/delta/pkg/usercom/types\"\n\tsubscriptions_types \"github.com/KosyanMedia/delta/pkg/usercom/types/subscriptions\"\n\tpakafka \"github.com/KosyanMedia/delta/price-alerts/internal/kafka\"\n\t\"github.com/KosyanMedia/gopkg/encoding/json\"\n\t\"github.com/KosyanMedia/gopkg/http/client/v2/transport\"\n\t\"github.com/KosyanMedia/gopkg/kafka\"\n\t\"github.com/KosyanMedia/gopkg/log\"\n)\n\nconst twoWeeks = 24 * time.Hour * 7 * 2\n\ntype Config struct {\n\tCommon        configs.CommonConfig\n\tRemoteConfigs configs.RemoteConfigsConfig\n\tKafkaAuth     configs.KafkaAuthConfig\n\tConsumer      struct {\n\t\tBrokers string `env:\"CONSUMER_KAFKA_BROKERS,required\"`\n\t\tTopic   string `env:\"CONSUMER_KAFKA_TOPIC,required\"`\n\t\tGroupID string `env:\"CONSUMER_KAFKA_GROUP_ID,required\"`\n\t}\n\tSubscriptionsAPI struct {\n\t\tEndpoint string `env:\"SUBSCRIPTIONS_API_ENDPOINT,required\"`\n\t}\n\tPhantomEnablePercentage struct {\n\t\tPercentage float64 `env:\"PHANTOM_ENABLE_PERCENTAGE,required\" envdefault:\"50.0\"`\n\t}\n\tUpdateInterval struct {\n\t\tInterval time.Duration `env:\"UPDATE_INTERVAL,required\" envdefault:\"300s\"`\n\t}\n}\n\ntype PhantomEnabledUserIDs struct {\n\tcfg atomic.Pointer[UserIDs]\n}\n\ntype UserIDs struct {\n\tIDs map[string]struct{}\n}\n\nfunc NewPhantomEnabledUserIDs() *PhantomEnabledUserIDs {\n\tp := &PhantomEnabledUserIDs{}\n\tp.cfg.Store(&UserIDs{IDs: make(map[string]struct{})})\n\treturn p\n}\n\nfunc (c *PhantomEnabledUserIDs) SetConfig(b []byte) error {\n\tvar ids []string\n\tif err := json.Unmarshal(b, &ids); err != nil {\n\t\treturn fmt.Errorf(\"parse user_ids config unmarshal json error: %w\", err)\n\t}\n\n\tconfig := &UserIDs{IDs: make(map[string]struct{}, len(ids))}\n\tfor _, id := range ids {\n\t\tconfig.IDs[id] = struct{}{}\n\t}\n\tc.cfg.Store(config)\n\n\treturn nil\n}\n\nfunc (c *PhantomEnabledUserIDs) Contains(userID string) bool {\n\tuserIDs := c.cfg.Load()\n\t_, ok := userIDs.IDs[userID]\n\treturn ok\n}\n\nfunc (c *PhantomEnabledUserIDs) GetIDs() []string {\n\tuserIDs := c.cfg.Load()\n\tvar ids []string\n\tfor id := range userIDs.IDs {\n\t\tids = append(ids, id)\n\t}\n\treturn ids\n}\n\ntype ExperimentsUpdater struct {\n\tusersProvider     *PhantomEnabledUserIDs\n\tunique            map[string]struct{}\n\tupdate            []string\n\tconfigAppEndpoint string\n\tc                 *http.Client\n\tmu                sync.Mutex\n\tupdateInterval    time.Duration\n}\n\nfunc NewGroupUpdater(client *http.Client, up *PhantomEnabledUserIDs, url string, updateInterval time.Duration) *ExperimentsUpdater {\n\treturn &ExperimentsUpdater{\n\t\tusersProvider:     up,\n\t\tunique:            make(map[string]struct{}),\n\t\tc:                 client,\n\t\tconfigAppEndpoint: url,\n\t\tupdateInterval:    updateInterval,\n\t}\n}\n\nfunc (e *ExperimentsUpdater) Run(ctx context.Context) {\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase <-time.After(e.updateInterval):\n\t\t\terr := e.updateExperiments(ctx)\n\t\t\tif err != nil {\n\t\t\t\tlog.Error(\"failed to update experiments\", log.Err(err))\n\t\t\t}\n\t\t}\n\n\t}\n}\n\nfunc (e *ExperimentsUpdater) AddUser(id string) {\n\te.mu.Lock()\n\tdefer e.mu.Unlock()\n\tif _, ok := e.unique[id]; ok {\n\t\treturn\n\t}\n\te.unique[id] = struct{}{}\n\te.update = append(e.update, id)\n}\n\nfunc (e *ExperimentsUpdater) updateExperiments(ctx context.Context) error {\n\te.mu.Lock()\n\tdefer e.mu.Unlock()\n\n\tupdate := append(e.usersProvider.GetIDs(), e.update...)\n\tdata, err := json.MarshalIndent(update, \"\", \"  \")\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treq, err := http.NewRequestWithContext(ctx, http.MethodPost, e.configAppEndpoint, bytes.NewBuffer(data))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treq.Header.Set(\"X-Email\", \"usercom.react@aviasales.ru\")\n\treq.Header.Set(\"Content-Type\", \"application/json\")\n\n\tresp, err := e.c.Do(req)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer resp.Body.Close()\n\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn fmt.Errorf(\"unexpected status code: %d\", resp.StatusCode)\n\t}\n\n\te.update = nil\n\treturn nil\n}\n\nfunc main() {\n\tvar cfg Config\n\tconfigs.MustParseContainer(&cfg)\n\n\tcmd.Initialize(cfg.Common)\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tremoteConfigs := remote_configs.New(cfg.Common.App, cfg.RemoteConfigs)\n\tphantomUsers := NewPhantomEnabledUserIDs()\n\tremoteConfigs.On(\"usercom/phantom_subscriptions_user_ids\", phantomUsers.SetConfig)\n\n\tgroupUpdater := NewGroupUpdater(&http.Client{Transport: transport.Default()},\n\t\tphantomUsers,\n\t\tcfg.RemoteConfigs.URL,\n\t\tcfg.UpdateInterval.Interval,\n\t)\n\tgo func() {\n\t\tgroupUpdater.Run(ctx)\n\t}()\n\n\tsubscriptionsAPI := subscriptions.NewClient(\n\t\t&http.Client{Transport: transport.Default()},\n\t\tcfg.SubscriptionsAPI.Endpoint,\n\t\tfalse,\n\t)\n\n\tconsumercfg := kafka.NewConsumerConfig(\"usercom.phantom-subscriptions-creator.aggregates-consumer\")\n\tif cfg.KafkaAuth.IsEnabled {\n\t\tconsumercfg.SASL(cfg.KafkaAuth.Username, cfg.KafkaAuth.Password)\n\t\tconsumercfg.TLSCert([]byte(cfg.KafkaAuth.PemCert))\n\t}\n\n\tconsumer := pakafka.NewConsumer(cfg.Consumer.Brokers,\n\t\tcfg.Consumer.GroupID,\n\t\tcfg.Consumer.Topic,\n\t\tconsumercfg,\n\t)\n\n\tconsumer.C.HandlePartitionMessage(func(message *kafka.ConsumerMessage, session kafka.ConsumerGroupSession) error {\n\t\tdefer session.MarkMessage(message)\n\n\t\tvar sr aggregates.SearchChainPayload\n\t\tif err := json.Unmarshal(message.Value, &sr); err != nil {\n\t\t\tlog.Error(\"could not unmarshal search result\", log.Err(err))\n\t\t\treturn err\n\t\t}\n\n\t\tif sr.SearchParams == nil || sr.SearchParams.UserData == nil || sr.SearchParams.UserData.Id == \"\" {\n\t\t\treturn nil\n\t\t}\n\n\t\tuserID := sr.SearchParams.UserData.Id\n\t\tlogger := log.With(log.String(\"user_id\", userID))\n\n\t\tif !phantomUsers.Contains(userID) {\n\t\t\tlogger.Debug(\"user_id does not belong to an experiment group\")\n\t\t\tchance := rand.Float64() * 100\n\t\t\tif chance < cfg.PhantomEnablePercentage.Percentage {\n\t\t\t\tlogger.Info(\"adding user to experiment group\")\n\t\t\t\tgroupUpdater.AddUser(userID)\n\t\t\t}\n\n\t\t\treturn nil\n\t\t}\n\n\t\tsubs, err := subscriptionsAPI.FetchUserSubscriptions(context.Background(), userID, types.BrandAviasales)\n\t\tif err != nil {\n\t\t\tlogger.Error(\"could not fetch user subscriptions\", log.Err(err))\n\t\t\treturn err\n\t\t}\n\t\tlogger.Info(\"received user subscriptions\", log.Int(\"subscriptions_count\", len(subs)))\n\n\t\tduplicatedID, err := checkDuplicates(subs, sr)\n\t\tif err != nil {\n\t\t\tlogger.Error(\"could not check duplicates\", log.Err(err))\n\t\t\treturn err\n\t\t}\n\t\tif duplicatedID != \"\" {\n\t\t\tlogger.Info(\"skipping search: subscription with such segments already exists\",\n\t\t\t\tlog.String(\"subscription_id\", duplicatedID),\n\t\t\t)\n\t\t\treturn nil\n\t\t}\n\n\t\tok, err := lastNotificationThresholdPassed(subscriptionsAPI, userID)\n\t\tif err != nil {\n\t\t\tlogger.Error(\"could not check last notification threshold\", log.Err(err))\n\t\t\treturn err\n\t\t}\n\t\tif !ok {\n\t\t\tlogger.Info(\"skipping search: less than 2 weeks passed since the last notification\")\n\t\t\treturn nil\n\t\t}\n\n\t\tcheck, err := maybeInvalidatePhantomSub(userID, subs, subscriptionsAPI)\n\t\tif err != nil {\n\t\t\tlogger.Error(\"could not invalidate phantom subscription\", log.Err(err))\n\t\t\treturn err\n\t\t}\n\t\tif check != nil {\n\t\t\tif !check.Invalidated {\n\t\t\t\tlogger.Info(\"skipping search: active phantom subscription already exists\",\n\t\t\t\t\tlog.String(\"active_phantom_subscription_id\", check.ActivePhantomID),\n\t\t\t\t)\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\tlogger.Info(\"invalidated phantom subscription\",\n\t\t\t\tlog.String(\"deleted_phantom_subscription_id\", check.DeletedPhantomID),\n\t\t\t)\n\t\t}\n\n\t\tif cerr := createPhantomSub(subscriptionsAPI, sr); cerr != nil {\n\t\t\tlogger.Error(\"could not create phantom subscription\", log.Err(cerr))\n\t\t\treturn cerr\n\t\t}\n\n\t\treturn nil\n\t})\n\n\tlauncher := app_service.NewLauncher(consumer, remoteConfigs)\n\tif err := launcher.Run(); err != nil {\n\t\tlog.Fatal(\"app start error\", log.Err(err))\n\t}\n}\n\nfunc createPhantomSub(api *subscriptions.Client, sr aggregates.SearchChainPayload) error {\n\treq := subscriptions_types.PostPhantomSubscriptionRequest{\n\t\tUserID:    sr.SearchParams.UserData.Id,\n\t\tTripClass: iata.NewTripClass(sr.SearchParams.TripClass.String()),\n\t}\n\n\tif p := sr.SearchParams.Passengers; p != nil {\n\t\treq.Adults = uint8(p.Adults)\n\t}\n\n\tif ua := sr.SearchParams.UserData.UserAgent; ua != nil {\n\t\treq.Marker = ua.Marker\n\t}\n\tsegments, err := getSegments(sr)\n\tif err != nil {\n\t\treturn err\n\t}\n\treq.Segments = segments\n\n\treturn api.PostPhantomSubscription(context.Background(), req)\n}\n\ntype InvalidationCheck struct {\n\tInvalidated      bool\n\tDeletedPhantomID string\n\tActivePhantomID  string\n}\n\nfunc maybeInvalidatePhantomSub(userID string, subs []*subscriptions_types.Subscription, api *subscriptions.Client) (*InvalidationCheck, error) {\n\tnow := time.Now()\n\tfor _, sub := range subs {\n\t\tif sub.Type != subscriptions_types.SubscriptionTypePhantomDirect {\n\t\t\tcontinue\n\t\t}\n\n\t\tif now.Sub(sub.CreatedAt) <= 3*time.Hour {\n\t\t\tif err := api.DeletePhantomSubscriptions(context.Background(), []string{userID}); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\treturn &InvalidationCheck{\n\t\t\t\tInvalidated:      true,\n\t\t\t\tDeletedPhantomID: sub.ID,\n\t\t\t}, nil\n\t\t}\n\n\t\treturn &InvalidationCheck{\n\t\t\tInvalidated:     false,\n\t\t\tActivePhantomID: sub.ID,\n\t\t}, nil\n\t}\n\n\treturn nil, nil\n}\n\nfunc lastNotificationThresholdPassed(api *subscriptions.Client, userID string) (bool, error) {\n\tnow := time.Now()\n\tuserData, err := api.GetUserData(context.Background(), userID)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\n\tif userData == nil {\n\t\treturn false, fmt.Errorf(\"no user data found: %s\", userID)\n\t}\n\n\tif userData.LastPhantomNotifiedAt.IsZero() {\n\t\treturn true, nil\n\t}\n\n\tif now.Sub(userData.LastPhantomNotifiedAt) <= twoWeeks {\n\t\treturn false, nil\n\t}\n\n\treturn true, nil\n}\n\nfunc checkDuplicates(subs []*subscriptions_types.Subscription, sr aggregates.SearchChainPayload) (string, error) {\n\tsegments, err := getSegments(sr)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tfor _, sub := range subs {\n\t\tif equalSegments(sub.Segments, segments) {\n\t\t\treturn sub.ID, nil\n\t\t}\n\t}\n\n\treturn \"\", nil\n}\n\nfunc getSegments(sr aggregates.SearchChainPayload) ([]subscriptions_types.RouteSegment, error) {\n\tsegments := make([]subscriptions_types.RouteSegment, 0, len(sr.SearchParams.Segments))\n\tfor _, segment := range sr.SearchParams.Segments {\n\t\tdate, err := datetime.ParseDate(segment.Date)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tsegments = append(segments, subscriptions_types.RouteSegment{\n\t\t\tOriginCity:      segment.Origin,\n\t\t\tDestinationCity: segment.Destination,\n\t\t\tDepartureDate:   date,\n\t\t})\n\t}\n\treturn segments, nil\n}\n\nfunc equalSegments(s1, s2 []subscriptions_types.RouteSegment) bool {\n\tif len(s1) != len(s2) {\n\t\treturn false\n\t}\n\tn := len(s1)\n\tfor i := 0; i < n; i++ {\n\t\ta := s1[i]\n\t\tb := s2[i]\n\t\tif a.OriginCity != b.OriginCity {\n\t\t\treturn false\n\t\t}\n\t\tif a.DestinationCity != b.DestinationCity {\n\t\t\treturn false\n\t\t}\n\t\tif a.DepartureDate != b.DepartureDate {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n```\n\nhow about now? what are some issues you can point out in this code?\n# Code Review of Updated Version\n\nThe code has been significantly improved based on the previous recommendations. Here are my thoughts on the current version:\n\n## Strengths\n- Thread safety has been properly implemented with mutex protection\n- Duplicate user ID checking is now handled with a map\n- Configuration via environment variables for update interval\n- Using the newer `math/rand/v2` package\n- Proper context handling for coordinated shutdown\n- Added mutex protection in critical sections\n\n## Remaining Issues and Additional Recommendations\n\n### 1. Inconsistent Deduplication Logic\n```go\nfunc (e *ExperimentsUpdater) updateExperiments(ctx context.Context) error {\n    e.mu.Lock()\n    defer e.mu.Unlock()\n\n    update := append(e.usersProvider.GetIDs(), e.update...)\n    // ...\n}\n```\n**Issue**: While `AddUser` prevents duplicates within `e.update`, there might still be duplicates between `e.update` and `GetIDs()`.\n\n**Recommendation**: \n```go\nfunc (e *ExperimentsUpdater) updateExperiments(ctx context.Context) error {\n    e.mu.Lock()\n    defer e.mu.Unlock()\n\n    // Deduplicate with a set\n    uniqueIDs := make(map[string]struct{})\n    for _, id := range e.usersProvider.GetIDs() {\n        uniqueIDs[id] = struct{}{}\n    }\n    for _, id := range e.update {\n        uniqueIDs[id] = struct{}{}\n    }\n    \n    // Convert to slice for marshaling\n    var update []string\n    for id := range uniqueIDs {\n        update = append(update, id)\n    }\n\n    data, err := json.MarshalIndent(update, \"\", \"  \")\n    // ...\n}\n```\n\n### 2. Potential Context Leaks in API Calls\n```go\nsubs, err := subscriptionsAPI.FetchUserSubscriptions(context.Background(), userID, types.BrandAviasales)\n```\n\n**Issue**: Using `context.Background()` in various API calls rather than a derived context with timeout.\n\n**Recommendation**: Create contexts with timeouts for all API calls:\n```go\nctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\ndefer cancel()\nsubs, err := subscriptionsAPI.FetchUserSubscriptions(ctx, userID, types.BrandAviasales)\n```\n\n### 3. Error Handling in `MarkMessage`\n```go\ndefer session.MarkMessage(message)\n```\n\n**Issue**: If message processing fails due to a temporary error, the message is still marked as processed, which could lead to data loss.\n\n**Recommendation**: Only mark the message when it's successfully processed or is unrecoverable:\n```go\nif err := processMessage(message); err != nil {\n    if isTemporaryError(err) {\n        return err // Don't mark, will be reprocessed\n    }\n    // Mark for permanent errors\n    session.MarkMessage(message)\n    return nil\n}\nsession.MarkMessage(message)\nreturn nil\n```\n\n### 4. Storage of Credentials\n```go\nreq.Header.Set(\"X-Email\", \"usercom.react@aviasales.ru\")\n```\n\n**Issue**: Hard-coded credentials in the code.\n\n**Recommendation**: Move this to configuration:\n```go\ntype Config struct {\n    // ...\n    ExperimentsAPI struct {\n        Email string `env:\"EXPERIMENTS_API_EMAIL,required\"`\n    }\n}\n\nreq.Header.Set(\"X-Email\", cfg.ExperimentsAPI.Email)\n```\n\n### 5. Error Propagation in Subscription Creation\n```go\nif cerr := createPhantomSub(subscriptionsAPI, sr); cerr != nil {\n    logger.Error(\"could not create phantom subscription\", log.Err(cerr))\n    return cerr\n}\n```\n\n**Issue**: Directly returning the error to Kafka, which may cause reprocessing attempts when the error is permanent.\n\n**Recommendation**: Categorize errors and handle them appropriately:\n```go\nif cerr := createPhantomSub(subscriptionsAPI, sr); cerr != nil {\n    logger.Error(\"could not create phantom subscription\", log.Err(cerr))\n    if isPermanentError(cerr) {\n        // Log but don't retry\n        return nil\n    }\n    return cerr // Only retry on temporary errors\n}\n```\n\n### 6. Potential Memory Growth\n```go\nfunc (e *ExperimentsUpdater) updateExperiments(ctx context.Context) error {\n    // ...\n    e.update = nil\n    return nil\n}\n```\n\n**Issue**: While the slice is being reset, the underlying array isn't freed. Over time with many additions, this could grow large.\n\n**Recommendation**: Properly reset with a new array periodically:\n```go\nif len(e.update) > someThreshold {\n    e.update = make([]string, 0, initialCapacity)\n} else {\n    e.update = e.update[:0] // Reuse the array if size is reasonable\n}\n```\n\n### 7. Resource Management for HTTP Client\n**Issue**: The HTTP client is created but never properly closed.\n\n**Recommendation**: Use a client with proper connection pooling and timeout configuration:\n```go\nclient := &http.Client{\n    Transport: &http.Transport{\n        MaxIdleConns:        100,\n        MaxIdleConnsPerHost: 100,\n        IdleConnTimeout:     90 * time.Second,\n    },\n    Timeout: 10 * time.Second,\n}\n```\n\n### 8. Context Usage in API Client Methods\n```go\nfunc createPhantomSub(api *subscriptions.Client, sr aggregates.SearchChainPayload) error {\n    // ...\n    return api.PostPhantomSubscription(context.Background(), req)\n}\n```\n\n**Issue**: Using `context.Background()` instead of passing the parent context.\n\n**Recommendation**: Pass the context from the parent function:\n```go\nfunc createPhantomSub(ctx context.Context, api *subscriptions.Client, sr aggregates.SearchChainPayload) error {\n    // ...\n    return api.PostPhantomSubscription(ctx, req)\n}\n\n// And call it with:\nif cerr := createPhantomSub(context.Background(), subscriptionsAPI, sr); cerr != nil {\n    // ...\n}\n```\n\n## Summary\nThe code has been significantly improved with proper concurrency handling and configuration. The remaining issues are primarily around error handling, context management, and resource optimization. Addressing these would further improve the robustness and maintainability of the code.\n","messages":[{"id":{"replica_id":0,"value":0},"start":0,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":397}}},{"id":{"replica_id":0,"value":12},"start":2047,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":398}}},{"id":{"replica_id":0,"value":28},"start":12763,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":399}}},{"id":{"replica_id":0,"value":29},"start":17182,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":400}}},{"id":{"replica_id":0,"value":223},"start":28478,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":395}}},{"id":{"replica_id":0,"value":224},"start":34122,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":396}}}],"summary":"Code Review of ExperimentsUpdater Implementation","slash_command_output_sections":[{"range":{"start":2047,"end":12761},"icon":"File","label":"delta/price-alerts/cmd/phantom-subscriptions-creator/main.go","metadata":{"path":"delta/price-alerts/cmd/phantom-subscriptions-creator/main.go"}},{"range":{"start":17182,"end":28409},"icon":"File","label":"delta/price-alerts/cmd/phantom-subscriptions-creator/main.go","metadata":{"path":"delta/price-alerts/cmd/phantom-subscriptions-creator/main.go"}}]}